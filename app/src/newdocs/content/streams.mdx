
import { Link } from 'react-router-dom'

import streamListing from './images/streams/stream-listing.png'
import streamSettings from './images/streams/setup-your-stream.png'

import docsStyles from '$newdocs/components/DocsLayout/docsLayout.pcss'
import links from '$shared/../links'
import CodeSnippet from '$shared/components/CodeSnippet'

import Warning from '$newdocs/components/Warning'

import MessagePartitioningImage from './images/streams/message-partitioning.png'
import StreamAndPartitionsImage from './images/streams/stream-and-partitions.png'

import {
    CreateJavascriptClient,
    CreateJavaClient,
    AuthJavascriptClient,
    AuthJavaClient,
    SubscribeJavascriptClient,
    SubscribeJavaClient,
    PublishJavascriptClient,
    PublishJavaClient,
} from './code/streams.js'

<section id="intro-to-streams">

## Intro to streams

All of the data in the Streamr network is contained inside individual streams. A stream is simply a sequence of data points in time. The data may originate, for example from machines on the factory floor, sensors in a smart city, in-house databases or systems, or from commercial streaming data feeds.

Streams have the following properties:
- Any kind of real-time data can be stored in it.
- The data will always be retrieved in the correct order.
- The data is persisted on the Streamr network and identifiable by unique ID.

Some common use cases for streams:
- Sensory readings of speed and orientation
- Geolocation
- Ambient temperature and humidity 
- Social media messages
- Stock market events
- Mobile ad impressions

These are just a few examples. There is virtually no limit on the kind, format, or quantity of data you can feed into Streamr, even digital images, streaming video, or other domain-specific data could be pushed into a stream.

Streams implement a publish-subscribe paradigm, or pub/sub for short. A stream can receive data from many sources (or publishers), and there can be several listeners who subscribe to a stream. There are several variations on the possible pub/sub topologies, such as many-to-one, one-to-many, or many-to-many. Streamr supports all of these.

### Example stream

Here’s an example of what a small part of a stream could look like. Each row shows one data point, and the columns correspond to the timestamp followed by two data fields, a measurement of the operating temperature and the number of rotations per minute (RPM).

| Timestamp                | Temperature   | RPM  |
| ------------------------ |:-------------:| ----:|
| 2019-08-01 11:30:01.012  | 312.56        | 3550 |
| 2019-08-01 11:30:02.239  | 312.49        | 3549 |
| 2019-08-01 11:30:04.105  | 312.42        | 3543 |
| 2016-02-01 11:30:08.122  | 313.21        | 3565 |
| 2016-02-01 11:30:11.882  | 317.45        | 3602 |

### Stream data fields

A field is a kind of placeholder for an individual piece of data, from a single data point. Each data point contains at least one data field, but you can have as many fields per data point as required.

For example, here's a data point in a stock market stream.

```
{
  "Symbol": "PFFT",
  "EventType": 1,
  "OrderId": 6454321,
  "Direction": "Up",
  "Trade": {"Price": 118.55, "Size": 100},
  "Ask": [
          {"Price": 118.6, "Size": 22500},
          {"Price": 118.65, "Size": 18000},
          {"Price": 118.7, "Size": 13000},
          {"Price": 118.8, "Size": 8000},
          {"Price": 119, "Size": 45000}
          ],
  "Bid": [
          {"Price": 118.5, "Size": 16500},
          {"Price": 118.45, "Size": 11000},
          {"Price": 118.4, "Size": 14200},
          {"Price": 118.2, "Size": 19000},
          {"Price": 118, "Size": 50000}
        ]
}
```

Given the structure of this data point, the stream's fields could be configured as follows:

| Field                | Field type    |
| -------------------- |:-------------:|
| Symbol               | String        |
| EventType            | Number        |
| OrderId              | Number        |
| Direction            | String        |
| Trade                | Object        |
| Ask                  | List          |
| Bid                  | List          |  

### Data field types

There’s a number of built-in data types that can be used in a stream. These are:

- **Number:** A numeric data type internally stored as a double precision (64-bit) float.
- **Boolean:** A logical data type with two possible values, True and False. In Streamr, a numeric value exactly equal to one represents logical truth. Anything else is interpreted as a logical falsehood.
- **String:** A sequence of zero or more alphabetical characters.
- **Object:** A collection of key-value pairs. Each key is a string, and the value can be of any built-in data type (even another object). Object is equivalent to Java's 'Map'.
- **List:** An ordered collection of zero or more elements. List is equivilent to an array.

Field types can be freely mixed in a single data point. And you can freely add new fields to an existing stream; you don’t have to know what fields you might eventually need. A single data point can be of any size within reason, and a stream can grow indefinitely when extended by new data points.

</section>

<section id="work-with-streams-in-core">

## Work with streams in Core
Streamr Core provides you with the tools to create, edit, share and delete your streams as well as view and inspect the streams you have purchased on the Marketplace.

### Overview of your streams
Head over to the <Link to={links.userpages.streams}>Streams page</Link> to see an overview of your subscribed streams. The list includes streams you have created and streams that you have purchased on the Marketplace.

<img src={streamListing} />

This stream listing page gives an indication for when data was last pushed to each stream and uses traffic light indicators show the health of each stream, at a glance.

| Color        | Status                           |
| ------------ | -------------------------------- |
| Green        | Stream is receiving data         |
| Red          | Stream is not receiving data     |

Clicking on a stream will open the stream editor. You may only edit streams that you have created or have explicit permission to edit.

By clicking on the more options icon you may access advanced features for each stream, such as stream sharing and code snippets.
A new stream can be created by the clicking the "Create Stream" button on the <Link to={links.userpages.streams}>Streams page</Link>.

### Editing a stream
Stream editor provides basic tools for setting up, inspecting and editing your stream.

<img src={streamSettings} />

#### Details section
The details section provides basic information about your stream. The stream name is used to identify your stream in the Core app.
The stream description can be used to offer more information about your stream and the kind of data it contains.
The stream ID is the unique identifier for your stream
and is needed when interacting with the <Link to={links.newdocs.SDKs}>Streamr SDKs</Link>.

#### Configure section
In this section you can configure stream fields and their data types. All fields have a name and a data type.
The Autodetect feature can be used to automatically deduce fields from the stream's last received message.
It is possible to manually correct the data types if automatic detection is not suitable for your stream.

| Setting                                             | Explanation                                                                                                     |
| ----------------------------------------------------| --------------------------------------------------------------------------------------------------------------- |
| Always try to autoconfigure field names and types   | Streamr will try to automatically configure field names and types on the very first message the stream receives |
| Require all messages in this stream to be signed    | Streamr will not accept messages that are not signed                                                            |

#### Preview section
This section is used to preview data flowing into the stream. Use the live inspector to make sure your stream is receiving data and it is correctly formatted.

#### API Access section
Here you can manage API keys for your stream. Keys generated here grant access to only this stream and can have either read or write access.
These keys can also be removed if for example, the key has been compromised.

#### Historical Data section
This section contains the stream's historical data storage settings and includes tools to add or remove historical data from the stream. 

You can add new data to the stream by uploading a CSV file and you can delete existing historical data from the stream by choosing an inclusive date range. 

The historical data storage period determines how many days your data is retained before it will be removed automatically from the Streamr network. The default period is 365 days.

</section>

<section id="work-with-streams-via-sdks">

## Work with streams via SDKs

The easiest way to work with streams is to use the JavaScript client, which works in `node.js` as well as the browser. See the <Link to={links.newdocs.SDKs}>SDKs section</Link> for the official and community-maintained client libraries available for various languages:

- **JavaScript** client works in the browser as well as node.js.
- **Java** client is work-in-progress but already implements the most important set of functionality.

Client libraries for other languages are on the <a href={links.community.trello} target="_blank" rel="nofollow noopener noreferrer">roadmap</a>. If a client library isn't available for your language, you can dive into the details of the [websocket protocol](https://github.com/streamr-dev/streamr-client-protocol-js/blob/bb2f0168bcff454b5c458e275e9c2de0dee342f9/PROTOCOL.md).

If you'd like to contribute a client library and get it listed here, please get in touch on <a href={links.community.telegram} target="_blank" rel="nofollow noopener noreferrer">Telegram</a>, <a href={links.community.reddit} target="_blank" rel="nofollow noopener noreferrer">Reddit</a> or <a href={links.contact.general} rel="nofollow noopener noreferrer">email</a>!

### Authentication

When reading from or writing to Streams, you need to provide a session token or an API key, or login with an Ethereum account. To read more about API keys or obtaining a session token, refer to the <Link to={links.newdocs.api}>Authentication section of the API documentation</Link>.

### Creating a client instance with API key

**Javascript**

<CodeSnippet language='javascript'>{CreateJavascriptClient}</CodeSnippet>

**Java**

<CodeSnippet language='java'>{CreateJavaClient}</CodeSnippet>

### Subscribing to real-time events in a stream

By subscribing to Streams, your application gets immediately notified about new events in the Stream.

**JavaScript**

<CodeSnippet language='javascript'>{SubscribeJavascriptClient}</CodeSnippet>

**Java**

<CodeSnippet language='java'>{SubscribeJavaClient}</CodeSnippet>

### Publishing events to Streams

JavaScript:

<CodeSnippet language='javascript'>{PublishJavascriptClient}</CodeSnippet>

Java:

Events in Streams are key-value pairs, represented in Java as Map objects. Below is an example of creating an event payload and publishing it into a Stream:

<CodeSnippet language='java'>{PublishJavaClient}</CodeSnippet>

</section>

<section id="work-with-streams-via-api">

## Work with streams via API
--content--

</section>

<section id="data-signing-and-verification">

## Data signing and verification

Authenticity and integrity of events published on a stream can be guaranteed with digital signatures. Every stream's metadata has a boolean `requiresSignedData` that can be set by the stream's owner to let subscribers know whether they should expect signed or unsigned events.

Every message published to a stream has six fields that uniquely identify this message across time, all streams and all publishers:

- `streamId`
- `streamPartititon`
- `timestamp`
- `sequenceNumber`
- `publisherId`
- `msgChainId`

More details about these fields can be found in the [protocol specification](TODO: insert link once merged). All together they form the message ID. They must be signed along with the actual message `content` to resist against replay attacks.

So the payload to be signed for every message by every publisher is the following:

```
payload = streamId + streamPartition + timestamp + sequenceNumber + publisherId + msgChaindId + content
```

The signing algorithm follows the convention described [here](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-712.md). The secp256k1 ECDSA algorithm is applied on the keccak256 hash of a string derived from the challenge text:

```
signature = sign(keccak256("\x19Ethereum Signed Message:\\n" + len(payload) + payload)))
```

On the recipient side, every subscriber needs to verify signed events. If a received event is unsigned, the subscriber accepts the event if and only if the stream's boolean flag `requiresSignedData` is set to `false`.

The signature verification is done in three steps:

1. the subscriber extracts from the event and the signature the Ethereum address that signed the message (using the EC recover operation).
2. Check that the address recovered in step 1 matches the address defined by `publisherId`
3. Check that this `publisherId` belongs to the set of valid publishers for that stream by querying the `api/v1/streams/${id}/publishers` endpoint.

Both signature computation and verification are implemented in the Javascript and Java SDKs.

</section>

<section id="end-to-end-encryption">

## End-to-end encryption
--content--

</section>

<section id="partitioning">

## Partitioning

To be precise, when messages are published to a stream, they are actually published to a **partition** within that stream. Partitions can be understood as sub-streams, or in other words, parts of a stream. One partition per stream is the default, which is sufficient for streams with moderate rates of data (approx. less than 100 msg/sec).

When a stream needs to handle large data rates, partitions are used for *sharding* of the data. Sharding data simply means dividing a large volume of messages to multiple partitions, a bit like a large river can split the same amount of water to multiple smaller branches. Each partition shares the general properties of the parent stream, such as name, description, and user permissions, but the partitions behave independently when it comes to delivering and storing data in the Streamr Network, which allows for scalability.

<img src={StreamAndPartitionsImage} />

Partitions also enable subscribers to scale horizontally: a user consuming data from a stream could load balance the messages over a number of consuming processes, up to the number of partitions. So if a stream has 5 partitions, the user could start up to 5 independent subscribers on separate physical machines, with each subscriber receiving different messages (each process subscribes to a unique partition).

Partitions are identified by a number which starts from zero. For example, a stream with only one partition contains the partition `0` and all of the stream's data flows through this partition. A stream with 5 partitions has partitions `0` through `4` - here the stream's data is distributed across these 5 partitions. The data publisher is free to choose the target partition for each message. If no partition is specified, the data goes to partition 0.

While the data publisher is free to choose a target partition for a message arbitrarily, a common approach is to utilize a *partition key*. A partition key is a value chosen from the data which is used to determine the partition of the message. For example, a customer ID could be used as a partition key in an application that publishes customer interactions to a stream. This way, all messages from a particular customer always go to the same partition. This is useful because it keeps all the events related to a particular customer in a single and known partition instead of spreading them over all partitions.

<img src={MessagePartitioningImage} />

The `publish` methods in the <Link to={links.newdocs.SDKs}>Streamr SDKs</Link> usually take an optional `partitionKey` parameter. For example, in the JS client:

```
streamr.publish(stream, message, timestamp, partitionKey)
```

Internally, the library maps the `partitionKey` to a partition number using a hash function (modulo the number of partitions in the stream):

```
partition = hash(partitionKey) % numberOfPartitions
```

<Warning>

At the time of writing, there are some limitations with multi-partition streams: The number of partitions can only be set when creating streams via the API, and the number cannot be changed afterwards. When using multi-partition streams on canvases, the canvases always subscribe to all partitions in a stream.

</Warning>

Currently partitions don’t have well-defined rate limits, but future versions of the P2P network may enforce limits. For now, we recommend not exceeding around 100 msg/sec/partition.


</section>

<section id="integration-patterns">

## Patterns for data integration

There are three distinct patterns for connecting data to Streamr, each with their own pros and cons:

- **1.** Pushing from the source (ideal)
- **2.** Bridging from a streaming source
- **3.** Bridging by polling a source

### 1. Pushing from the source

- Simplicity: **Good**
- Latency: **Good**
- Applicability: **Medium**

In this pattern, the data points are sent directly to Streamr by the source as soon as new data becomes available (for example, a sensor produces a new measurement).

**This is the recommended pattern**. However, depending on the circumstances, it may not always be available. It requires that you have control over the system that produces the data and can decide where it gets sent.

For example, if you are an IoT device manufacturer, you could add support for Streamr directly into your devices or gateways, enabling all your end users to easily connect their data to Streamr.

Various industrial data sources usually have good configurability for connecting the data anywhere you need to. However, consumer grade devices such as connected cars, phones, Fitbits, and others often force you to send your data to the manufacturer’s cloud, from where they might serve it back to you via an API. In these cases, you can use one of the bridging patterns in this post (see options two and three).

For a real-world example, here’s how to connect measurements from any number of <a target="_blank" href="https://ruuvi.com">Ruuvi</a> IoT tags to Streamr. The devices transmit data over Bluetooth Low Energy to a gateway computer which runs this node.js script. A separate stream is created for each found Ruuvi device, and the measurements are produced to the appropriate stream as they occur.

### 2. Bridging from a streaming source

- Simplicity: **Medium**
- Latency: **Good**
- Applicability: **Medium**

This pattern is useful when you don’t have direct control over the source, but a streaming API or some other kind of listener hook is available for the data. These allow you to be notified whenever new data occurs, enabling you to immediately forward the event to Streamr. Data that is typically available via these kinds of APIs are news feeds, financial market data and social media feeds.

### 3. Bridging by polling a source

- Simplicity: **Medium**
- Latency: **Medium**
- Applicability: **Good**

Most cloud services handing out data at least offer a request-response-based API, which can be used if neither of the above push-based options are available. In this case there is no way to get notified when new data is available, which means that the API needs to be repeatedly queried. This is called polling.

It’s not an optimal way to serve real-time data, because:

- Data can be missed: multiple value changes in between subsequent requests are not observed.
- It places an unnecessary load on the API server, as requests are made “just in case” regardless of whether the data has actually changed or not.
- An average latency of half the polling interval is introduced.

For more information and examples on using these integration patters, please see this deep dive <a target="_blank" href="https://medium.com/streamrblog/three-patterns-for-integrating-your-data-to-streamr-2-of-3-38027dc91f9e">blog post</a>. Please note that the examples used an older version of `streamr-client` and you should confirm with the <a target="_blank" href="https://github.com/streamr-dev/streamr-client-javascript">official docs</a> on the best practices. 
</section>
